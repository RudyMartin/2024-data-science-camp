{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNd5RHtMHBZgqibnZNRHXzy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RudyMartin/dsai-2024/blob/main/dsai_cifar10_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIFAR10 Version #3 with suggested changes"
      ],
      "metadata": {
        "id": "CcBQIwnj7m3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFn2q6kJ7WVL",
        "outputId": "a7c38322-9b21-4ddb-f537-98d7ef845fbc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 37433954.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "[Epoch 1, Batch 200] loss: 1.822\n",
            "[Epoch 2, Batch 200] loss: 1.390\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Data augmentation and normalization for training\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "# Just normalization for testing\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "class ImprovedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ImprovedNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(256 * 4 * 4, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "        self.bn3 = nn.BatchNorm2d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "net = ImprovedNet()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(50):  # Increase number of epochs\n",
        "    running_loss = 0.0\n",
        "    net.train()  # Set the model to training mode\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "        if i % 200 == 199:  # Print every 200 mini-batches\n",
        "            print(f'[Epoch {epoch + 1}, Batch {i + 1}] loss: {running_loss / 200:.3f}')\n",
        "            running_loss = 0.0\n",
        "    scheduler.step()\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "# Save the trained model\n",
        "PATH = './improved_cifar_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n",
        "\n",
        "# Testing loop\n",
        "correct = 0\n",
        "total = 0\n",
        "net.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n",
        "\n",
        "# Accuracy for each class\n",
        "correct_pred = {classname: 0 for classname in classes}\n",
        "total_pred = {classname: 0 for classname in classes}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predictions = torch.max(outputs, 1)\n",
        "        for label, prediction in zip(labels, predictions):\n",
        "            if label == prediction:\n",
        "                correct_pred[classes[label]] += 1\n",
        "            total_pred[classes[label]] += 1\n",
        "\n",
        "for classname, correct_count in correct_pred.items():\n",
        "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
        "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualization"
      ],
      "metadata": {
        "id": "mtPViLn87lEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the rest of your code has executed and you have your predictions and true labels\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming `net` is your trained model and `testloader` is your DataLoader for the test dataset\n",
        "# Make sure your model is in evaluation mode\n",
        "net.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        all_preds.extend(predicted.numpy())\n",
        "        all_labels.extend(labels.numpy())\n",
        "\n",
        "# Compute the confusion matrix\n",
        "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Plot the confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NLHJ-kGN7-zW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation Results"
      ],
      "metadata": {
        "id": "2PcC5jVL8Aqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming the rest of your code has executed and you have your predictions and true labels\n",
        "# import matplotlib.pyplot as plt\n",
        "# import numpy as np\n",
        "# import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score, f1_score, balanced_accuracy_score, matthews_corrcoef, hamming_loss, jaccard_score, cohen_kappa_score, classification_report\n",
        "import pandas as pd\n",
        "# import torch\n",
        "\n",
        "# Assuming `net` is your trained model and `testloader` is your DataLoader for the test dataset\n",
        "# Make sure your model is in evaluation mode\n",
        "# net.eval()\n",
        "\n",
        "# all_preds = []\n",
        "# all_labels = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         images, labels = data\n",
        "#         outputs = net(images)\n",
        "#         _, predicted = torch.max(outputs.data, 1)\n",
        "#         all_preds.extend(predicted.cpu().numpy())\n",
        "#         all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Convert all_preds and all_labels to numpy arrays\n",
        "all_preds = np.array(all_preds)\n",
        "all_labels = np.array(all_labels)\n",
        "\n",
        "# Ensure `classes` is a list of strings representing the class names\n",
        "#classes = [str(cls) for cls in classes]\n",
        "\n",
        "# Compute the confusion matrix\n",
        "# conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "# Initialize the metrics dictionary\n",
        "metrics = {\n",
        "    'Class': [],\n",
        "    'TP': [],\n",
        "    'FN': [],\n",
        "    'FP': [],\n",
        "    'TN': [],\n",
        "    'Recall': [],\n",
        "    'Precision': [],\n",
        "    'Accuracy': [],\n",
        "    'F1 Score': [],\n",
        "    'Specificity': [],\n",
        "    'MCC': [],\n",
        "    'Jaccard Index': []\n",
        "}\n",
        "\n",
        "# Calculate metrics for each class\n",
        "for i, class_label in enumerate(classes):\n",
        "    TP = conf_matrix[i, i]\n",
        "    FN = conf_matrix[i, :].sum() - TP\n",
        "    FP = conf_matrix[:, i].sum() - TP\n",
        "    TN = conf_matrix.sum() - (TP + FN + FP)\n",
        "\n",
        "    recall = TP / (TP + FN) if TP + FN != 0 else 0\n",
        "    precision = TP / (TP + FP) if TP + FP != 0 else 0\n",
        "    accuracy = (TP + TN) / (TP + TN + FP + FN) if TP + TN + FP + FN != 0 else 0\n",
        "    f1 = f1_score(all_labels, all_preds, labels=[i], average=None)[0]\n",
        "    specificity = TN / (TN + FP) if TN + FP != 0 else 0\n",
        "    mcc = matthews_corrcoef(all_labels, all_preds)\n",
        "    jaccard = jaccard_score(all_labels, all_preds, labels=[i], average=None)[0]\n",
        "\n",
        "    metrics['Class'].append(class_label)\n",
        "    metrics['TP'].append(TP)\n",
        "    metrics['FN'].append(FN)\n",
        "    metrics['FP'].append(FP)\n",
        "    metrics['TN'].append(TN)\n",
        "    metrics['Recall'].append(recall)\n",
        "    metrics['Precision'].append(precision)\n",
        "    metrics['Accuracy'].append(accuracy)\n",
        "    metrics['F1 Score'].append(f1)\n",
        "    metrics['Specificity'].append(specificity)\n",
        "    metrics['MCC'].append(mcc)\n",
        "    metrics['Jaccard Index'].append(jaccard)\n",
        "\n",
        "# Convert metrics dictionary to DataFrame\n",
        "df_metrics = pd.DataFrame(metrics)\n",
        "\n",
        "# Show the DataFrame with all metrics\n",
        "print(df_metrics)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df_metrics.to_csv('df_metrics_20240610.csv', index=False)"
      ],
      "metadata": {
        "id": "DVgtafHj8FWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Changes:\n",
        "Model Architecture:\n",
        "\n",
        "Added an additional convolutional layer.\n",
        "Included batch normalization and dropout layers.\n",
        "Data Augmentation:\n",
        "\n",
        "Added random cropping and horizontal flipping to the training data.\n",
        "Optimizer:\n",
        "\n",
        "Switched to the Adam optimizer for better performance.\n",
        "Learning Rate Scheduler:\n",
        "\n",
        "Added a learning rate scheduler to adjust the learning rate during training.\n",
        "Training Epochs:\n",
        "\n",
        "Increased the number of training epochs to 50."
      ],
      "metadata": {
        "id": "lkQrdP-_8uPT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xzybOb339Un0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Overview\n",
        "Data Loading and Normalization:\n",
        "\n",
        "Uses torchvision to load CIFAR-10 dataset.\n",
        "Applies normalization to transform the images.\n",
        "Model Architecture:\n",
        "\n",
        "A simple Convolutional Neural Network (CNN) with two convolutional layers followed by three fully connected layers.\n",
        "The network uses ReLU activations and max pooling.\n",
        "Training:\n",
        "\n",
        "Uses CrossEntropyLoss and SGD with momentum for optimization.\n",
        "Training loop runs for 2 epochs, iterating over the dataset.\n",
        "Evaluation:\n",
        "\n",
        "Model accuracy is computed on the test dataset.\n",
        "The script prints the accuracy for each class and the overall accuracy.\n",
        "Suggestions for Improvement\n",
        "Increase Model Complexity:\n",
        "\n",
        "More Convolutional Layers: Add more convolutional layers to capture more complex patterns.\n",
        "Batch Normalization: Add batch normalization layers to improve training stability and speed.\n",
        "Dropout: Add dropout layers to reduce overfitting.\n",
        "Data Augmentation:\n",
        "\n",
        "Apply data augmentation techniques (e.g., random horizontal flip, random crop) to increase the diversity of training data.\n",
        "Learning Rate Scheduling:\n",
        "\n",
        "Use a learning rate scheduler to adjust the learning rate during training, which can help in converging faster and avoiding local minima.\n",
        "Optimizer:\n",
        "\n",
        "Consider using more advanced optimizers like Adam, which often perform better than SGD with momentum.\n",
        "Training Epochs:\n",
        "\n",
        "Increase the number of training epochs. Two epochs are generally insufficient to fully train a model on CIFAR-10.\n",
        "Validation Set:\n",
        "\n",
        "Split the training data into training and validation sets to monitor the model’s performance on unseen data during training.\n",
        "Evaluation Metrics:\n",
        "\n",
        "In addition to accuracy, monitor precision, recall, and F1 score for a more comprehensive evaluation of the model performance.\n"
      ],
      "metadata": {
        "id": "OdxLGuN58vC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VwylvPpF8vMO"
      }
    }
  ]
}